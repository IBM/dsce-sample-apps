{
	"items": [
		{
		  "model_id": "bigscience/mt0-xxl",
		  "label": "mt0-xxl-13b",
		  "provider": "BigScience",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "An instruction-tuned iteration on mT5.",
		  "long_description": "mt0-xxl (13B) is an instruction-tuned iteration on mT5. Like BLOOMZ, it was fine-tuned on a cross-lingual task mixture dataset (xP3) using multitask prompted finetuning (MTF).",
		  "input_tier": "class_2",
		  "output_tier": "class_2",
		  "number_params": "13b",
		  "min_shot_size": 0,
		  "task_ids": [
			"question_answering",
			"summarization",
			"classification",
			"generation"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 3 } },
			{ "id": "summarization", "ratings": { "quality": 3 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 2 } },
			{ "id": "classification", "ratings": { "quality": 3 } },
			{ "id": "generation" },
			{ "id": "extraction", "ratings": { "quality": 2 } }
		  ],
		  "model_limits": { "max_sequence_length": 4096 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 700 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4095 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4095 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-07-07" }],
		  "price_input": 1.8,
		  "price_output": 1.8,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "codellama/codellama-34b-instruct-hf",
		  "label": "codellama-34b-instruct-hf",
		  "provider": "Code Llama",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.",
		  "long_description": "Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.",
		  "input_tier": "class_2",
		  "output_tier": "class_2",
		  "number_params": "34b",
		  "min_shot_size": 0,
		  "task_ids": ["code"],
		  "tasks": [{ "id": "code" }],
		  "model_limits": { "max_sequence_length": 16384 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 8192 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 8192 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 8192 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-03-14" }],
		  "price_input": 1.8,
		  "price_output": 1.8,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "google/flan-t5-xl",
		  "label": "flan-t5-xl-3b",
		  "provider": "Google",
		  "source": "Hugging Face",
		  "functions": [
			{ "id": "prompt_tune_inferable" },
			{ "id": "prompt_tune_trainable" },
			{ "id": "text_generation" }
		  ],
		  "short_description": "A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.",
		  "long_description": "flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "3b",
		  "min_shot_size": 0,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering" },
			{ "id": "summarization", "tags": ["function_prompt_tune_trainable"] },
			{ "id": "retrieval_augmented_generation" },
			{ "id": "classification", "tags": ["function_prompt_tune_trainable"] },
			{ "id": "generation", "tags": ["function_prompt_tune_trainable"] },
			{ "id": "extraction" }
		  ],
		  "model_limits": {
			"max_sequence_length": 4096,
			"training_data_max_records": 10000
		  },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4095 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4095 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4095 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-12-07" }],
		  "training_parameters": {
			"init_method": { "supported": ["random", "text"], "default": "random" },
			"init_text": { "default": "text" },
			"num_virtual_tokens": { "supported": [20, 50, 100], "default": 100 },
			"num_epochs": { "default": 20, "min": 1, "max": 50 },
			"verbalizer": { "default": "Input: {{input}} Output:" },
			"batch_size": { "default": 16, "min": 1, "max": 16 },
			"max_input_tokens": { "default": 256, "min": 1, "max": 256 },
			"max_output_tokens": { "default": 128, "min": 1, "max": 128 },
			"torch_dtype": { "default": "bfloat16" },
			"accumulate_steps": { "default": 16, "min": 1, "max": 128 },
			"learning_rate": { "default": 0.3, "min": 1e-5, "max": 0.5 }
		  },
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "google/flan-t5-xxl",
		  "label": "flan-t5-xxl-11b",
		  "provider": "Google",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.",
		  "long_description": "flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.",
		  "input_tier": "class_2",
		  "output_tier": "class_2",
		  "number_params": "11b",
		  "min_shot_size": 0,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 4 } },
			{ "id": "summarization", "ratings": { "quality": 4 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 3 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 4096 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 700 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4095 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4095 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-07-07" }],
		  "price_input": 1.8,
		  "price_output": 1.8,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "google/flan-ul2",
		  "label": "flan-ul2-20b",
		  "provider": "Google",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.",
		  "long_description": "flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.",
		  "input_tier": "class_3",
		  "output_tier": "class_3",
		  "number_params": "20b",
		  "min_shot_size": 0,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 4 } },
			{ "id": "summarization", "ratings": { "quality": 4 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 4 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 4096 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 700 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4095 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4095 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-07-07" }],
		  "price_input": 5.0,
		  "price_output": 5.0,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm-mistralai/merlinite-7b",
		  "label": "merlinite-7b",
		  "provider": "Mistral AI",
		  "tuned_by": "IBM",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.",
		  "long_description": "This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "7b",
		  "min_shot_size": 1,
		  "task_ids": [
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"code",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "summarization", "ratings": { "quality": 4 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 3 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "code" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 32768 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 8192 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 8192 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 8192 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-04-18" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-13b-chat-v2",
		  "label": "granite-13b-chat-v2",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "13b",
		  "min_shot_size": 0,
		  "task_ids": [
			"question_answering",
			"summarization",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 3 } },
			{ "id": "summarization", "ratings": { "quality": 2 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 2 } },
			{ "id": "classification", "ratings": { "quality": 3 } },
			{ "id": "generation" },
			{ "id": "extraction", "ratings": { "quality": 2 } }
		  ],
		  "model_limits": { "max_sequence_length": 8192 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 8191 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 8191 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 8191 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-12-01" }],
		  "versions": [
			{ "version": "2.1.0", "available_date": "2024-02-15" },
			{ "version": "2.0.0", "available_date": "2023-12-01" }
		  ],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-13b-instruct-v2",
		  "label": "granite-13b-instruct-v2",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [
			{ "id": "prompt_tune_inferable" },
			{ "id": "prompt_tune_trainable" },
			{ "id": "text_generation" }
		  ],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "13b",
		  "min_shot_size": 0,
		  "task_ids": [
			"question_answering",
			"summarization",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 3 } },
			{
			  "id": "summarization",
			  "ratings": { "quality": 2 },
			  "tags": ["function_prompt_tune_trainable"],
			  "training_parameters": {
				"init_method": { "supported": ["random", "text"], "default": "text" },
				"init_text": {
				  "default": "Please write a summary highlighting the main points of the following text:"
				},
				"num_virtual_tokens": { "supported": [20, 50, 100], "default": 100 },
				"num_epochs": { "default": 40, "min": 1, "max": 50 },
				"verbalizer": {
				  "default": "Please write a summary highlighting the main points of the following text: {{input}}"
				},
				"batch_size": { "default": 8, "min": 1, "max": 16 },
				"max_input_tokens": { "default": 256, "min": 1, "max": 1024 },
				"max_output_tokens": { "default": 128, "min": 1, "max": 512 },
				"torch_dtype": { "default": "bfloat16" },
				"accumulate_steps": { "default": 1, "min": 1, "max": 128 },
				"learning_rate": { "default": 0.0002, "min": 1e-5, "max": 0.5 }
			  }
			},
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 2 } },
			{
			  "id": "classification",
			  "ratings": { "quality": 3 },
			  "tags": ["function_prompt_tune_trainable"],
			  "training_parameters": {
				"init_method": { "supported": ["random", "text"], "default": "text" },
				"init_text": { "default": "Classify the text:" },
				"num_virtual_tokens": { "supported": [20, 50, 100], "default": 100 },
				"num_epochs": { "default": 20, "min": 1, "max": 50 },
				"verbalizer": { "default": "Input: {{input}} Output:" },
				"batch_size": { "default": 8, "min": 1, "max": 16 },
				"max_input_tokens": { "default": 256, "min": 1, "max": 1024 },
				"max_output_tokens": { "default": 128, "min": 1, "max": 512 },
				"torch_dtype": { "default": "bfloat16" },
				"accumulate_steps": { "default": 32, "min": 1, "max": 128 },
				"learning_rate": { "default": 0.0006, "min": 1e-5, "max": 0.5 }
			  }
			},
			{
			  "id": "generation",
			  "tags": ["function_prompt_tune_trainable"],
			  "training_parameters": {
				"init_method": { "supported": ["random", "text"], "default": "text" },
				"init_text": { "default": "text" },
				"num_virtual_tokens": { "supported": [20, 50, 100], "default": 100 },
				"num_epochs": { "default": 20, "min": 1, "max": 50 },
				"verbalizer": { "default": "{{input}}" },
				"batch_size": { "default": 16, "min": 1, "max": 16 },
				"max_input_tokens": { "default": 256, "min": 1, "max": 1024 },
				"max_output_tokens": { "default": 128, "min": 1, "max": 512 },
				"torch_dtype": { "default": "bfloat16" },
				"accumulate_steps": { "default": 16, "min": 1, "max": 128 },
				"learning_rate": { "default": 0.0002, "min": 1e-5, "max": 0.5 }
			  }
			},
			{ "id": "extraction", "ratings": { "quality": 2 } }
		  ],
		  "model_limits": {
			"max_sequence_length": 8192,
			"training_data_max_records": 10000
		  },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 8191 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 8191 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 8191 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-12-01" }],
		  "training_parameters": {
			"init_method": { "supported": ["random", "text"], "default": "random" },
			"init_text": { "default": "text" },
			"num_virtual_tokens": { "supported": [20, 50, 100], "default": 100 },
			"num_epochs": { "default": 20, "min": 1, "max": 50 },
			"verbalizer": { "default": "{{input}}" },
			"batch_size": { "default": 16, "min": 1, "max": 16 },
			"max_input_tokens": { "default": 256, "min": 1, "max": 1024 },
			"max_output_tokens": { "default": 128, "min": 1, "max": 512 },
			"torch_dtype": { "default": "bfloat16" },
			"accumulate_steps": { "default": 16, "min": 1, "max": 128 },
			"learning_rate": { "default": 0.0002, "min": 1e-5, "max": 0.5 }
		  },
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-20b-code-instruct",
		  "label": "granite-20b-code-instruct",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "20b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering" },
			{ "id": "summarization" },
			{ "id": "classification" },
			{ "id": "generation" },
			{ "id": "extraction" }
		  ],
		  "model_limits": { "max_sequence_length": 8192 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4096 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4096 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4096 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-05-06" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-20b-multilingual",
		  "label": "granite-20b-multilingual",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "20b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 3 } },
			{ "id": "summarization", "ratings": { "quality": 4 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 3 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 8192 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4096 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4096 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4096 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-03-14" }],
		  "versions": [
			{ "version": "1.1.0", "available_date": "2024-04-18" },
			{ "version": "1.0.0", "available_date": "2024-03-14" }
		  ],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-34b-code-instruct",
		  "label": "granite-34b-code-instruct",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "34b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering" },
			{ "id": "summarization" },
			{ "id": "classification" },
			{ "id": "generation" },
			{ "id": "extraction" }
		  ],
		  "model_limits": { "max_sequence_length": 16384 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 8192 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 8192 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 8192 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-05-06" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-3b-code-instruct",
		  "label": "granite-3b-code-instruct",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "3b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering" },
			{ "id": "summarization" },
			{ "id": "classification" },
			{ "id": "generation" },
			{ "id": "extraction" }
		  ],
		  "model_limits": { "max_sequence_length": 8192 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4096 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4096 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4096 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-05-09" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-7b-lab",
		  "label": "granite-7b-lab",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "7b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering" },
			{ "id": "summarization" },
			{ "id": "retrieval_augmented_generation" },
			{ "id": "classification" },
			{ "id": "generation" },
			{ "id": "extraction" }
		  ],
		  "model_limits": { "max_sequence_length": 4096 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4095 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4095 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4095 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-04-18" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "ibm/granite-8b-code-instruct",
		  "label": "granite-8b-code-instruct",
		  "provider": "IBM",
		  "source": "IBM",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
		  "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "8b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"classification",
			"generation",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering" },
			{ "id": "summarization" },
			{ "id": "classification" },
			{ "id": "generation" },
			{ "id": "extraction" }
		  ],
		  "model_limits": { "max_sequence_length": 8192 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4096 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4096 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4096 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-05-09" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "meta-llama/llama-2-13b-chat",
		  "label": "llama-2-13b-chat",
		  "provider": "Meta",
		  "source": "Hugging Face",
		  "functions": [
			{ "id": "prompt_tune_inferable" },
			{ "id": "prompt_tune_trainable" },
			{ "id": "text_generation" }
		  ],
		  "short_description": "Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.",
		  "long_description": "Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "13b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"code",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 4 } },
			{
			  "id": "summarization",
			  "ratings": { "quality": 3 },
			  "tags": ["function_prompt_tune_trainable"]
			},
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 4 } },
			{
			  "id": "classification",
			  "ratings": { "quality": 4 },
			  "tags": ["function_prompt_tune_trainable"]
			},
			{ "id": "generation", "tags": ["function_prompt_tune_trainable"] },
			{ "id": "code" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": {
			"max_sequence_length": 4096,
			"training_data_max_records": 10000
		  },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 2048 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4095 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4095 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-11-09" }],
		  "training_parameters": {
			"init_method": { "supported": ["random", "text"], "default": "random" },
			"init_text": { "default": "text" },
			"num_virtual_tokens": { "supported": [20, 50, 100], "default": 100 },
			"num_epochs": { "default": 20, "min": 1, "max": 50 },
			"verbalizer": { "default": "{{input}}" },
			"batch_size": { "default": 8, "min": 1, "max": 16 },
			"max_input_tokens": { "default": 256, "min": 1, "max": 1024 },
			"max_output_tokens": { "default": 128, "min": 1, "max": 512 },
			"torch_dtype": { "default": "bfloat16" },
			"accumulate_steps": { "default": 16, "min": 1, "max": 128 },
			"learning_rate": { "default": 0.002, "min": 1e-5, "max": 0.5 }
		  },
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "meta-llama/llama-2-70b-chat",
		  "label": "llama-2-70b-chat",
		  "provider": "Meta",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.",
		  "long_description": "Llama-2-70b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.",
		  "input_tier": "class_2",
		  "output_tier": "class_2",
		  "number_params": "70b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"code",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 4 } },
			{ "id": "summarization", "ratings": { "quality": 3 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 4 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "code" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 4096 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 900 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4095 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4095 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2023-09-07" }],
		  "price_input": 1.8,
		  "price_output": 1.8,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "meta-llama/llama-3-70b-instruct",
		  "label": "llama-3-70b-instruct",
		  "provider": "Meta",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.",
		  "long_description": "Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.",
		  "input_tier": "class_2",
		  "output_tier": "class_2",
		  "number_params": "70b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"code",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 4 } },
			{ "id": "summarization", "ratings": { "quality": 3 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 4 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "code" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 8192 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4096 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4096 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4096 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-04-18" }],
		  "price_input": 1.8,
		  "price_output": 1.8,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "meta-llama/llama-3-8b-instruct",
		  "label": "llama-3-8b-instruct",
		  "provider": "Meta",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.",
		  "long_description": "Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "8b",
		  "min_shot_size": 1,
		  "task_ids": [
			"question_answering",
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"code",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "question_answering", "ratings": { "quality": 4 } },
			{ "id": "summarization", "ratings": { "quality": 3 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 4 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "code" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 8192 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 4096 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 4096 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 4096 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-04-18" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		},
		{
		  "model_id": "mistralai/mixtral-8x7b-instruct-v01",
		  "label": "mixtral-8x7b-instruct-v01",
		  "provider": "Mistral AI",
		  "source": "Hugging Face",
		  "functions": [{ "id": "text_generation" }],
		  "short_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
		  "long_description": "This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.",
		  "input_tier": "class_1",
		  "output_tier": "class_1",
		  "number_params": "46.7b",
		  "min_shot_size": 1,
		  "task_ids": [
			"summarization",
			"retrieval_augmented_generation",
			"classification",
			"generation",
			"code",
			"extraction"
		  ],
		  "tasks": [
			{ "id": "summarization", "ratings": { "quality": 4 } },
			{ "id": "retrieval_augmented_generation", "ratings": { "quality": 3 } },
			{ "id": "classification", "ratings": { "quality": 4 } },
			{ "id": "generation" },
			{ "id": "code" },
			{ "id": "extraction", "ratings": { "quality": 4 } }
		  ],
		  "model_limits": { "max_sequence_length": 32768 },
		  "limits": {
			"lite": { "call_time": "5m0s", "max_output_tokens": 16384 },
			"v2-professional": { "call_time": "10m0s", "max_output_tokens": 16384 },
			"v2-standard": { "call_time": "10m0s", "max_output_tokens": 16384 }
		  },
		  "lifecycle": [{ "id": "available", "start_date": "2024-04-17" }],
		  "price_input": 0.6,
		  "price_output": 0.6,
		  "platform": "watsonx",
		  "pricing_page": "https://www.ibm.com/products/watsonx-ai/foundation-models#generative"
		}
	  ]

}
